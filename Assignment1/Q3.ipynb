{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0     1     2     3     4     5     6     7     8     9    ...    114  \\\n0  1.0  0.19  0.33  0.02  0.90  0.12  0.17  0.34  0.47  0.29  ...   0.12   \n1  1.0  0.00  0.16  0.12  0.74  0.45  0.07  0.26  0.59  0.35  ...   0.02   \n2  1.0  0.00  0.42  0.49  0.56  0.17  0.04  0.39  0.47  0.28  ...   0.01   \n3  1.0  0.04  0.77  1.00  0.08  0.12  0.10  0.51  0.50  0.34  ...   0.02   \n4  1.0  0.01  0.55  0.02  0.95  0.09  0.05  0.38  0.38  0.23  ...   0.04   \n\n    115   116   117   118  119  120   121   122   123  \n0  0.26  0.20  0.06  0.04  0.9  0.5  0.32  0.14  0.20  \n1  0.12  0.45   NaN   NaN  NaN  NaN  0.00   NaN  0.67  \n2  0.21  0.02   NaN   NaN  NaN  NaN  0.00   NaN  0.43  \n3  0.39  0.28   NaN   NaN  NaN  NaN  0.00   NaN  0.12  \n4  0.09  0.02   NaN   NaN  NaN  NaN  0.00   NaN  0.03  \n\n[5 rows x 124 columns]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "3) Real-life dataset\n",
    "'''\n",
    "\n",
    "df = pd.read_csv('./Datasets/communities.csv', header=None)\n",
    "df[0] = 1\n",
    "df.drop([i for i in range(1, 5)], axis=1, inplace=True) # These columns are not predictive according to the dataset\n",
    "df.columns = [i for i in range(df.shape[1])] # Rename columns\n",
    "df = df.replace('?', np.NaN).astype(np.float64)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0     1     2     3     4     5     6     7     8     9    ...    114  \\\n268   1.0  0.00  0.62  0.03  0.95  0.08  0.01  1.00  1.00  1.00  ...   0.01   \n261   1.0  0.02  0.23  0.44  0.64  0.03  0.04  0.25  0.30  0.20  ...   0.03   \n732   1.0  0.01  0.41  0.66  0.43  0.09  0.08  0.37  0.47  0.30  ...   0.01   \n562   1.0  0.05  0.43  0.11  0.82  0.34  0.08  0.25  0.36  0.21  ...   0.03   \n1397  1.0  0.05  0.77  0.75  0.39  0.07  0.03  0.42  0.66  0.40  ...   0.06   \n\n       115   116       117       118       119       120   121       122   123  \n268   0.20  0.00  0.163103  0.076708  0.698589  0.440439  0.00  0.195078  0.05  \n261   0.17  0.03  0.163103  0.076708  0.698589  0.440439  0.00  0.195078  0.59  \n732   0.41  0.21  0.163103  0.076708  0.698589  0.440439  0.00  0.195078  0.66  \n562   0.27  0.43  0.010000  0.020000  0.850000  1.000000  0.43  0.250000  0.12  \n1397  0.15  0.03  0.163103  0.076708  0.698589  0.440439  0.00  0.195078  0.60  \n\n[5 rows x 124 columns]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Part 1) Fill in missing values \n",
    "'''\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "# print(df.columns[df.isnull().any()].tolist())\n",
    "\n",
    "N_examples = df.shape[0]\n",
    "M_cols = df.shape[1]\n",
    "df = df.sample(frac=1) # This shuffles the examples\n",
    "data = np.array(df, dtype=np.float64)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Part 2) Fit data and report 5-fold cross-validation error\n",
    "'''\n",
    "# Normal equation with regularization (lambda_reg=0 means no regularization)\n",
    "def fit(X, y, lambda_reg=0):\n",
    "    identity = np.identity(X.shape[1])\n",
    "    # identity[0, 0] = 0 # We do not penalize the bias term\n",
    "\n",
    "    X_square = np.matmul(np.transpose(X), X) + lambda_reg * identity\n",
    "    X_square_inverse = np.linalg.pinv(X_square)\n",
    "    weights = np.matmul(np.matmul(X_square_inverse, np.transpose(X)), y)\n",
    "\n",
    "    return weights\n",
    "\n",
    "# Gradient descent\n",
    "def gradient_descent(X, y, lambda_reg=0, alpha=1e-2, epochs=5000, weights=None):\n",
    "    '''\n",
    "    Implementation of vectorized gradient descent with L2 regularization support\n",
    "    :param X: The input matrix N x M, where each row is an example\n",
    "    :param y: The output, N x 1\n",
    "    :param lambda_reg: Regularization hyperparamter (0 means no regularization)\n",
    "    :param alpha: Learning rate\n",
    "    :param epochs: Number of cycles over training data\n",
    "    :param weights: Initial weights can be supplied if desired\n",
    "    :return: The optimal weights after gradient descent\n",
    "    '''\n",
    "    weights = weights if weights is not None else np.random.uniform(high=10, size=[M_cols - 1])\n",
    "    N = len(X)\n",
    "    for epoch in range(epochs):\n",
    "        weights = weights - alpha / N * ( np.matmul(np.transpose(X), np.matmul(X, weights) - y) + lambda_reg * weights)\n",
    "\n",
    "    return weights\n",
    "\n",
    "def mean_square_error(X, y, W):\n",
    "    y_hat = np.matmul(X, W)\n",
    "    mean_square_err = np.sum(np.square(y - y_hat)) / len(y)\n",
    "\n",
    "    return mean_square_err\n",
    "\n",
    "\n",
    "def cross_validation_split(X, n_folds=5, filename=\"file\", write_to_csv=False):\n",
    "    '''\n",
    "    Splits the dataset intn n_folds\n",
    "    :param X: The input matrix, N x M\n",
    "    :param n_folds: The number of folds\n",
    "    :param filename: The output file prefix\n",
    "    :param write_to_csv: True if saving each fold required\n",
    "    :return: An array of dictionaries of length n_folds\n",
    "    '''\n",
    "    N = len(X) // n_folds\n",
    "    pairs = []\n",
    "    for i in range(n_folds):\n",
    "        fold_train1 = X[0:i * N]\n",
    "        if i < n_folds - 1:\n",
    "            fold_test = X[i*N:(i+1)*N]\n",
    "            fold_train2 = X[(i+1)*N:]\n",
    "        else:\n",
    "            fold_test = X[i*N:]\n",
    "            fold_train2 = X[N:N]\n",
    "\n",
    "        df_train = pd.DataFrame(np.concatenate((fold_train1, fold_train2)))\n",
    "        df_test = pd.DataFrame(fold_test)\n",
    "\n",
    "        if write_to_csv:\n",
    "            df_train.to_csv('./Datasets/' + filename + '-train' + str(i + 1) + '.csv', header=False, index=False)\n",
    "            df_test.to_csv('./Datasets/' + filename + '-test' + str(i + 1) + '.csv', header=False, index=False)\n",
    "\n",
    "        pairs.append({'train': df_train, 'test': df_test})\n",
    "\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def write_to_csv(to_df_object, filename, indexname=None):\n",
    "    df_to_csv = pd.DataFrame(to_df_object)\n",
    "    if indexname:\n",
    "        df_to_csv.index.name = indexname\n",
    "    df_to_csv.to_csv(filename + '.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the files for 5-fold cross-validation\n",
    "# splits = [{'train':<train_data>, 'test': <test_data>}, {'train':<train_data>, 'test': <test_data>}, {'train':<train_data>, 'test': <test_data>}\\\n",
    "# {'train':<train_data>, 'test': <test_data>}, {'train':<train_data>, 'test': <test_data>}]\n",
    "splits = cross_validation_split(data, 5, 'CandC', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSEs_closed_form = []\n",
    "all_weights = {}\n",
    "fold = 1\n",
    "for pair in splits:\n",
    "    X_train = pair['train'].drop([M_cols - 1], axis=1)\n",
    "    y_train = pair['train'][M_cols-1]\n",
    "    X_test = pair['test'].drop([M_cols - 1], axis=1)\n",
    "    y_test = pair['test'][M_cols - 1]\n",
    "\n",
    "    weights = fit(X_train, y_train)\n",
    "    MSEs_closed_form.append(mean_square_error(X_test, y_test, weights))\n",
    "    all_weights['fold' + str(fold)] = weights\n",
    "    fold += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSEs_gd = []\n",
    "all_weights_gd = {}\n",
    "weights = np.random.uniform(high=10., size=[M_cols - 1])\n",
    "fold = 1\n",
    "for pair in splits:\n",
    "    X_train = pair['train'].drop([M_cols - 1], axis=1)\n",
    "    y_train = pair['train'][M_cols - 1]\n",
    "    X_test = pair['test'].drop([M_cols - 1], axis=1)\n",
    "    y_test = pair['test'][M_cols - 1]\n",
    "\n",
    "    weights_gd = gradient_descent(np.array(X_train), np.array(y_train), epochs=20000, weights=np.copy(weights))\n",
    "    MSEs_gd.append(mean_square_error(X_test, y_test, weights_gd))\n",
    "    all_weights_gd['fold' + str(fold)] = weights_gd\n",
    "    fold += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "least squares: 0.11375623270993196\ngradient descent: 0.4515071630259898\n"
     ]
    }
   ],
   "source": [
    "print('least squares:', np.average(MSEs_closed_form))\n",
    "print('gradient descent:', np.average(MSEs_gd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The assignment wants us to store all these weights so the TAs can look over them.\n",
    "# Write weights to file (weight_index refers to w0, w1, etc. The weights for each fold are represented as columns in the csv. There are 5 columns and 123 rows)\n",
    "write_to_csv(all_weights, 'q3_part2_weights_for_closed_form', 'weight_index')\n",
    "write_to_csv(all_weights_gd, 'q3_part2_weights_for_gradient_descent', 'weight_index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best lambda: 1.4500000000000002\nMSE for best lambda: 0.01840859569971876\nMSE for lambda = 0.0 is 0.11375623270993196\nMSE for lambda = 0.05 is 0.018783559531641635\nMSE for lambda = 0.1 is 0.018671102139699434\nMSE for lambda = 0.15000000000000002 is 0.018606628386696335\nMSE for lambda = 0.2 is 0.01856429472896397\nMSE for lambda = 0.25 is 0.018534154811722287\nMSE for lambda = 0.30000000000000004 is 0.018511509491807975\nMSE for lambda = 0.35000000000000003 is 0.018493841133968483\nMSE for lambda = 0.4 is 0.018479675377742474\nMSE for lambda = 0.45 is 0.01846808824435594\nMSE for lambda = 0.5 is 0.01845846856524503\nMSE for lambda = 0.55 is 0.01845039383862847\nMSE for lambda = 0.6000000000000001 is 0.018443561060671652\nMSE for lambda = 0.65 is 0.01843774609950309\nMSE for lambda = 0.7000000000000001 is 0.01843277874878928\nMSE for lambda = 0.75 is 0.018428526813205958\nMSE for lambda = 0.8 is 0.018424885612707026\nMSE for lambda = 0.8500000000000001 is 0.018421770855258663\nMSE for lambda = 0.9 is 0.018419113669920696\nMSE for lambda = 0.9500000000000001 is 0.018416857064574233\nMSE for lambda = 1.0 is 0.018414953346760826\nMSE for lambda = 1.05 is 0.018413362210505886\nMSE for lambda = 1.1 is 0.018412049293037386\nMSE for lambda = 1.1500000000000001 is 0.018410985069321963\nMSE for lambda = 1.2000000000000002 is 0.018410143993534914\nMSE for lambda = 1.25 is 0.0184095038239037\nMSE for lambda = 1.3 is 0.018409045085607756\nMSE for lambda = 1.35 is 0.018408750639038653\nMSE for lambda = 1.4000000000000001 is 0.018408605329383095\nMSE for lambda = 1.4500000000000002 is 0.01840859569971876\nMSE for lambda = 1.5 is 0.01840870975419836\nMSE for lambda = 1.55 is 0.018408936761110783\nMSE for lambda = 1.6 is 0.018409267087987354\nMSE for lambda = 1.6500000000000001 is 0.018409692062643083\nMSE for lambda = 1.7000000000000002 is 0.01841020385539331\nMSE for lambda = 1.75 is 0.018410795378662116\nMSE for lambda = 1.8 is 0.018411460200985694\nMSE for lambda = 1.85 is 0.018412192472983653\nMSE for lambda = 1.9000000000000001 is 0.01841298686336488\nMSE for lambda = 1.9500000000000002 is 0.018413838503354853\nMSE for lambda = 2.0 is 0.01841474293827543\nMSE for lambda = 2.0500000000000003 is 0.018415696085170605\nMSE for lambda = 2.1 is 0.018416694195629097\nMSE for lambda = 2.15 is 0.01841773382302051\nMSE for lambda = 2.2 is 0.018418811793572826\nMSE for lambda = 2.25 is 0.018419925180737422\nMSE for lambda = 2.3000000000000003 is 0.018421071282415075\nMSE for lambda = 2.35 is 0.018422247600672702\nMSE for lambda = 2.4000000000000004 is 0.018423451823634918\nMSE for lambda = 2.45 is 0.018424681809266333\nMSE for lambda = 2.5 is 0.018425935570829305\nMSE for lambda = 2.5500000000000003 is 0.018427211263818917\nMSE for lambda = 2.6 is 0.018428507174173634\nMSE for lambda = 2.6500000000000004 is 0.018429821707651606\nMSE for lambda = 2.7 is 0.018431153380214924\nMSE for lambda = 2.75 is 0.018432500809323023\nMSE for lambda = 2.8000000000000003 is 0.01843386270602862\nMSE for lambda = 2.85 is 0.018435237867793282\nMSE for lambda = 2.9000000000000004 is 0.018436625171950233\nMSE for lambda = 2.95 is 0.018438023569740246\nMSE for lambda = 3.0 is 0.018439432080865327\nMSE for lambda = 3.0500000000000003 is 0.01844084978851236\nMSE for lambda = 3.1 is 0.01844227583479304\nMSE for lambda = 3.1500000000000004 is 0.018443709416563574\nMSE for lambda = 3.2 is 0.01844514978158886\nMSE for lambda = 3.25 is 0.018446596225014683\nMSE for lambda = 3.3000000000000003 is 0.018448048086126684\nMSE for lambda = 3.35 is 0.018449504745352285\nMSE for lambda = 3.4000000000000004 is 0.018450965621514735\nMSE for lambda = 3.45 is 0.018452430169277695\nMSE for lambda = 3.5 is 0.018453897876800324\nMSE for lambda = 3.5500000000000003 is 0.018455368263556895\nMSE for lambda = 3.6 is 0.018456840878323957\nMSE for lambda = 3.6500000000000004 is 0.018458315297313854\nMSE for lambda = 3.7 is 0.01845979112244054\nMSE for lambda = 3.75 is 0.018461267979710178\nMSE for lambda = 3.8000000000000003 is 0.018462745517731215\nMSE for lambda = 3.85 is 0.018464223406318557\nMSE for lambda = 3.9000000000000004 is 0.018465701335206197\nMSE for lambda = 3.95 is 0.018467179012837686\nMSE for lambda = 4.0 is 0.018468656165243458\nMSE for lambda = 4.05 is 0.018470132534994577\nMSE for lambda = 4.1000000000000005 is 0.01847160788022404\nMSE for lambda = 4.15 is 0.018473081973713158\nMSE for lambda = 4.2 is 0.01847455460203699\nMSE for lambda = 4.25 is 0.018476025564766647\nMSE for lambda = 4.3 is 0.018477494673719407\nMSE for lambda = 4.3500000000000005 is 0.01847896175226186\nMSE for lambda = 4.4 is 0.018480426634650663\nMSE for lambda = 4.45 is 0.01848188916541738\nMSE for lambda = 4.5 is 0.01848334919879443\nMSE for lambda = 4.55 is 0.018484806598171283\nMSE for lambda = 4.6000000000000005 is 0.01848626123558344\nMSE for lambda = 4.65 is 0.018487712991238473\nMSE for lambda = 4.7 is 0.01848916175306377\nMSE for lambda = 4.75 is 0.018490607416284024\nMSE for lambda = 4.800000000000001 is 0.01849204988302487\nMSE for lambda = 4.8500000000000005 is 0.018493489061936035\nMSE for lambda = 4.9 is 0.01849492486784155\nMSE for lambda = 4.95 is 0.01849635722140357\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Part 3) Ridge-regression: Using only least-squares closed form solution for this part.\n",
    "'''\n",
    "INCREMENTS = 100\n",
    "MSE_vs_lambda = []\n",
    "lambdas = np.arange(0, 5, 5 / INCREMENTS)\n",
    "lowest_mse = 999999\n",
    "best_lambda = -1\n",
    "\n",
    "for lambda_value in lambdas:\n",
    "    cur_mse = 0\n",
    "    weights_for_folds = {}\n",
    "    fold = 0\n",
    "    for pair in splits:\n",
    "        X_train = pair['train'].drop([M_cols - 1], axis=1)\n",
    "        y_train = pair['train'][M_cols - 1]\n",
    "        X_test = pair['test'].drop([M_cols - 1], axis=1)\n",
    "        y_test = pair['test'][M_cols - 1]\n",
    "\n",
    "        weights = fit(X_train, y_train, lambda_value)\n",
    "        cur_mse += mean_square_error(X_test, y_test, weights)\n",
    "        weights_for_folds['fold' + str(fold)] = weights\n",
    "\n",
    "    cur_mse /= len(splits)\n",
    "    if cur_mse < lowest_mse:\n",
    "        lowest_mse = cur_mse\n",
    "        best_lambda = lambda_value\n",
    "\n",
    "    MSE_vs_lambda.append(cur_mse)\n",
    "    write_to_csv(weights_for_folds, './question3part3/weights_lambda_' + str(lambda_value), 'weight_index')\n",
    "\n",
    "print('Best lambda:', best_lambda)\n",
    "print('MSE for best lambda:', lowest_mse)\n",
    "\n",
    "for i in range(len(MSE_vs_lambda)):\n",
    "    print('MSE for lambda =', lambdas[i], 'is', MSE_vs_lambda[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x32199d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "plt.plot(lambdas[1:], MSE_vs_lambda[1:])\n",
    "plt.title('Lambda vs MSE')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('MSE')\n",
    "plt.savefig('./plots/q3p31',  bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "# If a feature's associated weight is close to zero, it means that it contributes less to the prediction.\n",
    "# These features are good candidates for being excluded from the model.\n",
    "# We can first sort the weights and see which ones contribute the least to prediction\n",
    "all_weights_feature_selection = {}\n",
    "fold = 1\n",
    "for pair in splits:\n",
    "    X_train = pair['train'].drop([M_cols - 1], axis=1)\n",
    "    y_train = pair['train'][M_cols - 1]\n",
    "    X_test = pair['test'].drop([M_cols - 1], axis=1)\n",
    "    y_test = pair['test'][M_cols - 1]\n",
    "\n",
    "    weights = fit(X_train, y_train, lambda_reg=best_lambda) # Regularization with best_lambda from above\n",
    "    all_weights_feature_selection['fold' + str(fold)] = weights\n",
    "    fold += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(115, 8.124623147717888e-05), (67, 0.0004643035816042154), (66, 0.0009268880150553495), (113, 0.0016187454639819156), (110, 0.0016657493436844444), (61, 0.0020845037441557844), (57, 0.004027100557292785), (48, 0.004113091604215479), (82, 0.0042878916674042285), (53, 0.004589961740674946), (94, 0.0050013271680434445), (2, 0.006913839117046524), (21, 0.007166470581476536), (54, 0.007562558408070376), (42, 0.008243028917512532), (114, 0.009559699391876833), (96, 0.009911657251553673), (56, 0.010187368721822456), (5, 0.01028776127154761), (77, 0.0107419023588264), (88, 0.010830565271037299), (121, 0.011356460279119978), (9, 0.01213055136764094), (97, 0.012198452207720601), (81, 0.012896766439455972), (112, 0.013386084156584566), (79, 0.014705549220279049), (28, 0.015171412820434781), (117, 0.01530937402517533), (108, 0.017052788541819307), (50, 0.01709241123450979), (107, 0.017851295621492746), (104, 0.018123239923388667), (98, 0.0181522020991548), (33, 0.01831195940336018), (84, 0.018317260147567432), (85, 0.018823854497400143), (74, 0.019528977292411136), (119, 0.019602159890509088), (93, 0.020705132441375996), (47, 0.02083525206010371), (60, 0.021851546332153467), (122, 0.022129775717193535), (27, 0.024013914378485407), (44, 0.024510263187694302), (43, 0.024961895030467057), (11, 0.02889387084673674), (25, 0.030185036868812014), (55, 0.03086457690099268), (118, 0.031080950353316022), (105, 0.0314867147961331), (58, 0.03226280793305644), (26, 0.03314988530523295), (1, 0.03427799566668629), (36, 0.03445537682678858), (23, 0.03445975274575207), (7, 0.034717941145441085), (4, 0.03495368028378858), (71, 0.035782960651237414), (102, 0.038244860368687024), (78, 0.03863164988361926), (12, 0.04047673504025442), (120, 0.0405854198442865), (24, 0.041699672801100825), (95, 0.041703646911551644), (15, 0.04399395441987664), (18, 0.04407582407452883), (75, 0.045970293287849635), (116, 0.04787773825019125), (17, 0.04794530451987602), (13, 0.04902187877554586), (32, 0.04911124971775431), (46, 0.05216205582961707), (59, 0.05476939170270016), (64, 0.05516685050733309), (106, 0.05524847809594299), (20, 0.055870192903652256), (63, 0.05624963042206152), (68, 0.059550055534628756), (73, 0.06247068966956129), (109, 0.06309663799236623), (72, 0.06321380945821772), (19, 0.06395360340487759), (35, 0.06421218834330708), (6, 0.0653932884841878), (87, 0.06588553836343884), (76, 0.06603872930993208), (101, 0.06710776853839338), (100, 0.06739649320289469), (37, 0.07083571652142376), (92, 0.0708380167023913), (80, 0.07516844974278897), (14, 0.07679018891077856), (103, 0.07779945981640608), (89, 0.0809489455692209), (41, 0.08174512056691112), (0, 0.0876716845183411), (29, 0.08857648348882578), (99, 0.0957108886081992), (70, 0.09967017086904309), (10, 0.09994132700754993), (38, 0.09996368843564071), (90, 0.10400549765675042), (69, 0.10931709324923689), (52, 0.11025167398127245), (30, 0.11040951697080906), (111, 0.1133437434121179), (8, 0.11604319064657928), (65, 0.11923659433072083), (16, 0.12049267947068794), (31, 0.1210225904477362), (49, 0.12800714934329305), (86, 0.12954338847553287), (40, 0.12982290717434974), (45, 0.13768780966504252), (22, 0.14638331439930338), (91, 0.14800138903945592), (62, 0.14883060169004175), (51, 0.1569460792069627), (34, 0.15871658809825318), (83, 0.17171945204800396), (39, 0.17755664492884127), (3, 0.19971041150777075)]\n[(115, 0.0005349166411644294), (77, 0.0007961121689064355), (114, 0.0016253642442679178), (47, 0.0019618549453554344), (82, 0.0022016700153669987), (84, 0.003271619299728191), (106, 0.00563521427823988), (96, 0.005779368152772554), (36, 0.006234758674750359), (79, 0.006734051912866299), (111, 0.007569684361738306), (97, 0.008221788682246005), (23, 0.008718623902851521), (1, 0.009206031361697368), (122, 0.01000900941688462), (32, 0.010366673024556359), (9, 0.010886706306167929), (60, 0.012659614969741126), (56, 0.013185277937629235), (88, 0.013428935606168863), (71, 0.013437010361865752), (7, 0.013774468133609719), (18, 0.014104891670295295), (5, 0.014406570363184872), (94, 0.014892351591654286), (110, 0.015497938374716386), (98, 0.016608723463375705), (28, 0.01713531586871398), (108, 0.017338142134872135), (81, 0.017502569743071707), (67, 0.017570534964373694), (104, 0.01781760048398524), (78, 0.018464092811045274), (21, 0.019205658226206256), (112, 0.019347634380173718), (57, 0.019787049087182734), (55, 0.02038347642394043), (93, 0.02047816095909239), (74, 0.021161170985139813), (109, 0.021187028966635473), (107, 0.021448867934007906), (58, 0.02262517094068798), (37, 0.0236450986837807), (53, 0.025122819342143814), (119, 0.025555349569698932), (25, 0.027523404115663147), (118, 0.028193935424712067), (54, 0.028829280863540308), (43, 0.02979414578480877), (113, 0.030075801120992486), (42, 0.03040133784126807), (102, 0.03071820200873379), (13, 0.031089752838861), (48, 0.031941999635184744), (33, 0.03224640703445564), (76, 0.03271894763683206), (116, 0.033039381755189445), (120, 0.03365379377340976), (35, 0.03432432020992905), (85, 0.03484555685873075), (52, 0.036023628203030865), (12, 0.036424038254878506), (27, 0.03655190897500048), (61, 0.03712745751407899), (64, 0.037587504688132774), (15, 0.03762198771787043), (121, 0.03834379245549322), (44, 0.03840585317254909), (24, 0.03967059028073529), (100, 0.040003818864968316), (101, 0.04048674607000304), (19, 0.040752306939749086), (31, 0.0410293595979712), (59, 0.04132378994887018), (26, 0.044233921868483864), (50, 0.047101762493485494), (73, 0.047127978079877265), (41, 0.04734119243525106), (38, 0.05042667208702186), (2, 0.05083205683102917), (11, 0.051311735077624245), (4, 0.051345279993086225), (95, 0.052688700384653675), (63, 0.053692551046113014), (70, 0.053923205758273716), (80, 0.05725394910793833), (10, 0.057867586500683764), (6, 0.05988757235797935), (99, 0.06031338026541967), (14, 0.06258252550575474), (62, 0.0633434785877191), (87, 0.06424172509151806), (20, 0.06615775348933663), (89, 0.06683934846304497), (66, 0.07060617756316344), (117, 0.07307026708642768), (105, 0.0752647294098671), (75, 0.07586128588305979), (30, 0.07705026485896327), (22, 0.07710758200130502), (68, 0.08149661960878876), (0, 0.08287787584293814), (103, 0.0844384464716996), (40, 0.0865783406032569), (45, 0.08817855313180066), (90, 0.08965508414552363), (8, 0.08979104529228284), (46, 0.09474772684179543), (17, 0.09646066982469266), (29, 0.09946792393402991), (92, 0.10063499286593339), (16, 0.11078883977269237), (34, 0.1186001154601548), (65, 0.12712493621596424), (49, 0.13020919128289948), (72, 0.13446245587216185), (39, 0.14452031688451983), (91, 0.14931183384319477), (86, 0.15237665105484957), (3, 0.1564860771048652), (69, 0.15801794389624352), (83, 0.1686667953273362), (51, 0.18900853141894836)]\n[(33, 0.0010112980249820545), (115, 0.0017814657335523845), (94, 0.0025221252277814886), (42, 0.0033314151750626067), (77, 0.004391190609619076), (106, 0.00523668483762622), (5, 0.005535304536479111), (60, 0.0071372687761987455), (56, 0.007189121478052676), (81, 0.007369292981597732), (93, 0.007953884346321168), (61, 0.008091500063505055), (36, 0.008460458268205094), (79, 0.00956752754920256), (67, 0.009808800865687183), (96, 0.010193733364740998), (111, 0.011552634585705997), (58, 0.012528590529926742), (98, 0.012645624378105697), (82, 0.012851994117476197), (102, 0.013158198406481894), (104, 0.013967162605411618), (54, 0.01447396215991524), (28, 0.014659694308557068), (27, 0.015390452331740492), (71, 0.01602840179415679), (47, 0.01632901897447085), (85, 0.01786281888295903), (84, 0.020605121090875755), (31, 0.020624084458080293), (112, 0.020697578247829515), (113, 0.021290708735758185), (2, 0.021429118331112742), (24, 0.021565893705147825), (78, 0.02215217653183339), (53, 0.02249386895394138), (97, 0.022765895273353865), (55, 0.022777144949165554), (108, 0.023590923485131466), (74, 0.02364072573389903), (107, 0.0241065386892176), (110, 0.024638649970102414), (50, 0.024982112456669238), (121, 0.025421740211607462), (23, 0.025991544189717775), (21, 0.028775064341926924), (116, 0.02956729923059887), (26, 0.029903538447581462), (48, 0.030236351592093737), (117, 0.03026247812334956), (7, 0.033315859766961456), (4, 0.03336391538631273), (57, 0.03474877732330463), (15, 0.03540380409753896), (59, 0.03580428605388608), (66, 0.036237609430301215), (63, 0.038277616651579034), (120, 0.03932314849198645), (1, 0.03955154043075103), (20, 0.039683421526604146), (32, 0.03983019244221507), (118, 0.04041757361030216), (12, 0.04060611744060452), (46, 0.041045857156132425), (25, 0.04179212899312046), (75, 0.041900006376976384), (88, 0.041999480779994394), (52, 0.042949488980503214), (43, 0.04387254399771057), (119, 0.04448702180993864), (95, 0.0456082961339279), (11, 0.046101255651647716), (6, 0.048031431331724675), (109, 0.049162514480021804), (80, 0.05007565464811223), (9, 0.05228762900005309), (13, 0.05368367032742639), (103, 0.05528039254638155), (114, 0.05529077445797966), (38, 0.05540726229782463), (18, 0.0555069683987942), (100, 0.05609398736617508), (87, 0.05805815433694818), (76, 0.058070661514017075), (35, 0.05933707895622551), (73, 0.06517754849231984), (105, 0.0653058246903474), (44, 0.06547151603049015), (68, 0.0661713452518399), (122, 0.06694849514804527), (14, 0.06715145520195401), (101, 0.0671583472515696), (41, 0.06732292143369432), (30, 0.06946115581061137), (10, 0.06968398512833954), (62, 0.06993141079276843), (19, 0.07104945201359719), (64, 0.0776515529881093), (17, 0.07845659153043885), (92, 0.08191041152576925), (89, 0.08370847103929772), (37, 0.08400340262389723), (70, 0.08613339410181747), (90, 0.08889004375909404), (99, 0.10371193722829068), (8, 0.10505152325684826), (65, 0.10641967899535117), (22, 0.1086976222765025), (40, 0.1155828568946105), (29, 0.11895252649936608), (0, 0.1214603626521924), (49, 0.13012072178417405), (69, 0.13163707738423905), (16, 0.13312712900717594), (72, 0.1367781820472712), (39, 0.1374171909051961), (34, 0.14177261435947655), (86, 0.1519477062430324), (91, 0.15458862359820258), (51, 0.154908161747637), (45, 0.16567499127350044), (3, 0.17256271323824396), (83, 0.1728863053119854)]\n[(42, 0.00024972146253762924), (78, 0.000922469742719835), (33, 0.0011171528496846563), (38, 0.002118750728913912), (81, 0.0038406422552426872), (4, 0.0048787855006083795), (97, 0.005176669134232167), (106, 0.005348021810240442), (93, 0.006575461825409819), (110, 0.0068947010790204035), (77, 0.006925962185443454), (36, 0.00814023898439163), (107, 0.0083663064286818), (115, 0.00881313010306736), (46, 0.008825923509138116), (1, 0.010253375859800292), (58, 0.011650380999833707), (67, 0.011789307646289977), (25, 0.012063695102593742), (119, 0.013076592755978398), (5, 0.014993470005424513), (71, 0.016058697556330496), (85, 0.017109865540978236), (57, 0.017793327073448103), (74, 0.019089246054044634), (96, 0.01953747157183267), (24, 0.019661299266378465), (56, 0.019960999899188772), (54, 0.020129987199840365), (23, 0.020155534614772833), (79, 0.022050359073511792), (84, 0.02310886584261282), (61, 0.024935062429574203), (20, 0.025644744297959935), (53, 0.02579151075397383), (15, 0.026118336332802108), (118, 0.02614055212729375), (50, 0.027097825876486934), (100, 0.027152932170494137), (121, 0.028348357845284496), (21, 0.029653437860893337), (13, 0.03033203201230683), (48, 0.03035410960082219), (95, 0.03183833835857819), (112, 0.03290537674072487), (11, 0.033541581746600525), (55, 0.03356645954170078), (28, 0.034059362476687165), (120, 0.03486524606887565), (114, 0.03561314109079243), (113, 0.03578591567089757), (27, 0.036046415859305674), (102, 0.036335612720866745), (116, 0.03642072323804402), (2, 0.03897230302706431), (12, 0.04300333681176978), (94, 0.043818035983859555), (7, 0.044447084415674215), (111, 0.044509501783218046), (101, 0.04477341823783209), (87, 0.04534142853954116), (88, 0.04601792335655799), (47, 0.04863777012800525), (105, 0.049482434122276785), (82, 0.04971163239583885), (18, 0.05018195689429175), (108, 0.05050279704171125), (26, 0.05129760174803014), (63, 0.05350917292122173), (60, 0.05362148227323369), (31, 0.05427539415365504), (98, 0.054498080985226544), (35, 0.054719308945425836), (75, 0.05497920314365442), (104, 0.055639060039872554), (41, 0.05850673565708298), (9, 0.058936833551095545), (76, 0.05973719208387292), (37, 0.06290361998317186), (64, 0.06473270633687825), (92, 0.06518255949583783), (66, 0.06623065805642203), (59, 0.06813026339066752), (117, 0.07126449423190502), (30, 0.0730849864795721), (32, 0.07361698738626504), (122, 0.0742488733279744), (80, 0.07459802850961646), (19, 0.07556382784677343), (43, 0.07579603237799135), (10, 0.07732799997467892), (6, 0.07758379024863628), (17, 0.07840033480364862), (103, 0.07893308036169712), (14, 0.07928755120104271), (89, 0.0792960660867179), (73, 0.08253523638460596), (109, 0.08346586612247857), (22, 0.08420234398607299), (70, 0.08465482656301591), (68, 0.08985868932304666), (99, 0.09105604454115818), (29, 0.09966919031601024), (44, 0.09983574697303534), (62, 0.10143998841804241), (69, 0.10311893626987965), (45, 0.10531559294282757), (49, 0.10985839271400262), (0, 0.11214059748023214), (8, 0.11699498510856408), (52, 0.12037211331161794), (40, 0.12192803757211962), (16, 0.1247482914833034), (34, 0.13306602686104269), (39, 0.13372743829695594), (91, 0.1351505515985044), (72, 0.1368189802101664), (90, 0.1418653969582324), (86, 0.1446273565973415), (51, 0.14688548319373884), (65, 0.16885814034587635), (83, 0.17362070142089361), (3, 0.19062823273050525)]\n[(106, 0.0005832615613111779), (94, 0.0009086322144877912), (93, 0.0010492214219720655), (28, 0.0025843153303227438), (77, 0.004045455993406497), (67, 0.004475297495007928), (79, 0.004663011606214548), (108, 0.005016098171416439), (71, 0.005472550104445061), (33, 0.005750519035878848), (58, 0.007608065517215979), (113, 0.009670704049217473), (104, 0.0097429793851114), (57, 0.01024397735285012), (54, 0.010452693403796988), (98, 0.010912108946012798), (120, 0.011869645622931241), (110, 0.013793225920279237), (42, 0.013926690422420595), (56, 0.01476155449981957), (47, 0.015515509148055197), (61, 0.015712341003267065), (21, 0.015741370583601363), (18, 0.01596637119963718), (27, 0.016333264316973465), (7, 0.016885564444616714), (119, 0.017020311207680577), (95, 0.01830383030943075), (36, 0.018637985225284075), (50, 0.019944902805569303), (81, 0.020039778282439046), (112, 0.02139729308191972), (118, 0.021479253916066535), (96, 0.021904512991422466), (97, 0.023037851257713217), (115, 0.023844628989805716), (24, 0.024539858487150953), (25, 0.02476948377053892), (48, 0.025179071795231156), (60, 0.025591827800322375), (107, 0.02567963784709979), (37, 0.026151067358680048), (105, 0.026171226971436524), (13, 0.028501914276470836), (84, 0.02852196564355186), (55, 0.028534640022789968), (43, 0.029889363197214653), (121, 0.03079121604810247), (116, 0.03144194944603374), (23, 0.032047516896090186), (15, 0.03216567947527078), (122, 0.03231446633559876), (38, 0.03247284152112293), (35, 0.032653685121716784), (53, 0.03274215030669418), (74, 0.03275951908581642), (88, 0.03343413469775098), (2, 0.03507427821132269), (85, 0.03569507761918232), (82, 0.03591538522451186), (46, 0.037457052765749344), (78, 0.038015138595395896), (101, 0.03822562507658647), (100, 0.04066056919581853), (32, 0.0427953721781229), (41, 0.04304350126985955), (111, 0.04324679468135619), (12, 0.04344834565694757), (73, 0.043627775327734875), (114, 0.04508094038627617), (64, 0.04579980361560897), (14, 0.04584701920536539), (109, 0.04676015354830635), (20, 0.04771148537370331), (117, 0.049087437801526614), (102, 0.049576158949146375), (59, 0.049785891008216836), (44, 0.05022853157661314), (66, 0.05101489028623658), (1, 0.05536404840825471), (11, 0.05558925848888488), (26, 0.05592867322293985), (76, 0.05656336822623131), (6, 0.05671303034127711), (5, 0.057176520579048574), (75, 0.058519343933756206), (103, 0.0634660690419141), (31, 0.0638032267608), (87, 0.06387650423465832), (92, 0.06521088809484067), (9, 0.06942409206508238), (63, 0.07373527196008466), (80, 0.07417570629717404), (17, 0.08098193671802463), (30, 0.08100637139850086), (19, 0.0868247137831669), (70, 0.08984104613227781), (89, 0.09192077895568879), (10, 0.09201296539015492), (52, 0.09481651521165763), (68, 0.0955304244058701), (69, 0.0974473500579816), (4, 0.09751213632531808), (90, 0.10451792346420923), (62, 0.10542102699879573), (99, 0.10598557091791287), (16, 0.10765851757228387), (29, 0.11560797727048759), (86, 0.117482999939418), (8, 0.11956666759057248), (34, 0.11986893686666365), (39, 0.12303773469373071), (40, 0.12934132637307544), (49, 0.12948636199324332), (22, 0.13067177491033707), (51, 0.1387916880227916), (45, 0.14400921742756106), (0, 0.1482560770040247), (3, 0.1510822836718754), (91, 0.16026267698450294), (65, 0.16969259489229638), (83, 0.1708820808248538), (72, 0.1711133499794022)]\n"
     ]
    }
   ],
   "source": [
    "# Now we look for weights that are close to zero in the regularized fit\n",
    "features_to_drop = set()\n",
    "for key in all_weights_feature_selection:\n",
    "    w = all_weights_feature_selection[key]\n",
    "    # This basically converts the weight vector [w0, w1...] to [(0, w0), (1, w1),...] so after we sort the weights we know which features they correspond to\n",
    "    all_weights_feature_selection[key] = [(i, abs(w[i])) for i in range(len(w))]\n",
    "    all_weights_feature_selection[key].sort(key=lambda x: x[1])\n",
    "    [features_to_drop.add(x[0]) for x in all_weights_feature_selection[key][:20]]\n",
    "\n",
    "for val in all_weights_feature_selection.values():\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features to be dropped: 51\nBest lambda: 1.4500000000000002\nMSE for best lambda: 0.01840859569971876\nMSE for best lambda after dropping least contributing features: 0.01791864486507244\n"
     ]
    }
   ],
   "source": [
    "MSEs_final = []\n",
    "all_weights_final = {}\n",
    "fold = 1\n",
    "for pair in splits:\n",
    "    X_train = pair['train'].drop([M_cols - 1], axis=1)\n",
    "    y_train = pair['train'][M_cols-1]\n",
    "    X_test = pair['test'].drop([M_cols - 1], axis=1)\n",
    "    y_test = pair['test'][M_cols - 1]\n",
    "\n",
    "    # Drop the features we found to have near 0 coefficients\n",
    "    X_train.drop(list(features_to_drop), axis=1, inplace=True)\n",
    "    X_test.drop(list(features_to_drop), axis=1, inplace=True)\n",
    "\n",
    "    weights = fit(X_train, y_train, lambda_reg=best_lambda)\n",
    "    MSEs_final.append(mean_square_error(X_test, y_test, weights))\n",
    "    all_weights_final['fold' + str(fold)] = weights\n",
    "    fold += 1\n",
    "\n",
    "print('Number of features to be dropped:', len(features_to_drop))\n",
    "print('Best lambda:', best_lambda)\n",
    "print('MSE for best lambda:', lowest_mse)\n",
    "print('MSE for best lambda after dropping least contributing features:', np.average(MSEs_final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

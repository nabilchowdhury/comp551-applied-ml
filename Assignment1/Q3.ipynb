{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0     1     2     3     4     5     6     7     8     9    ...    114  \\\n0  1.0  0.19  0.33  0.02  0.90  0.12  0.17  0.34  0.47  0.29  ...   0.12   \n1  1.0  0.00  0.16  0.12  0.74  0.45  0.07  0.26  0.59  0.35  ...   0.02   \n2  1.0  0.00  0.42  0.49  0.56  0.17  0.04  0.39  0.47  0.28  ...   0.01   \n3  1.0  0.04  0.77  1.00  0.08  0.12  0.10  0.51  0.50  0.34  ...   0.02   \n4  1.0  0.01  0.55  0.02  0.95  0.09  0.05  0.38  0.38  0.23  ...   0.04   \n\n    115   116   117   118  119  120   121   122   123  \n0  0.26  0.20  0.06  0.04  0.9  0.5  0.32  0.14  0.20  \n1  0.12  0.45   NaN   NaN  NaN  NaN  0.00   NaN  0.67  \n2  0.21  0.02   NaN   NaN  NaN  NaN  0.00   NaN  0.43  \n3  0.39  0.28   NaN   NaN  NaN  NaN  0.00   NaN  0.12  \n4  0.09  0.02   NaN   NaN  NaN  NaN  0.00   NaN  0.03  \n\n[5 rows x 124 columns]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "3) Real-life dataset\n",
    "'''\n",
    "\n",
    "df = pd.read_csv('./Datasets/communities.csv', header=None)\n",
    "df[0] = 1\n",
    "df.drop([i for i in range(1, 5)], axis=1, inplace=True) # These columns are not predictive according to the dataset\n",
    "df.columns = [i for i in range(df.shape[1])] # Rename columns\n",
    "df = df.replace('?', np.NaN).astype(np.float64)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0     1     2     3     4     5     6     7     8     9    ...    114  \\\n425   1.0  0.01  0.49  0.09  0.85  0.10  0.13  0.43  0.53  0.33  ...   0.01   \n1309  1.0  0.10  0.37  0.02  0.97  0.04  0.07  0.26  0.30  0.17  ...   0.31   \n1747  1.0  0.14  0.60  0.14  0.69  0.23  0.28  0.41  0.55  0.31  ...   0.26   \n563   1.0  0.04  0.33  0.33  0.73  0.02  0.01  0.47  0.50  0.42  ...   0.07   \n93    1.0  0.00  0.49  0.01  0.94  0.14  0.04  0.47  0.42  0.27  ...   0.00   \n\n       115   116       117       118       119       120  121       122   123  \n425   0.26  0.02  0.163103  0.076708  0.698589  0.440439  0.0  0.195078  0.16  \n1309  0.06  0.02  0.163103  0.076708  0.698589  0.440439  0.0  0.195078  0.13  \n1747  0.09  0.06  0.163103  0.076708  0.698589  0.440439  0.0  0.195078  0.50  \n563   0.13  0.00  0.163103  0.076708  0.698589  0.440439  0.0  0.195078  0.25  \n93    0.35  0.20  0.163103  0.076708  0.698589  0.440439  0.0  0.195078  0.12  \n\n[5 rows x 124 columns]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Part 1) Fill in missing values \n",
    "'''\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "# print(df.columns[df.isnull().any()].tolist())\n",
    "\n",
    "N_examples = df.shape[0]\n",
    "M_cols = df.shape[1]\n",
    "df = df.sample(frac=1) # This shuffles the examples\n",
    "data = np.array(df, dtype=np.float64)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Part 2) Fit data and report 5-fold cross-validation error\n",
    "'''\n",
    "# Normal equation with regularization (lambda_reg=0 means no regularization)\n",
    "def fit(X, y, lambda_reg=0):\n",
    "    identity = np.identity(X.shape[1])\n",
    "    # identity[0, 0] = 0 # We do not penalize the bias term\n",
    "\n",
    "    X_square = np.matmul(np.transpose(X), X) + lambda_reg * identity\n",
    "    X_square_inverse = np.linalg.pinv(X_square)\n",
    "    weights = np.matmul(np.matmul(X_square_inverse, np.transpose(X)), y)\n",
    "\n",
    "    return weights\n",
    "\n",
    "# Gradient descent\n",
    "def gradient_descent(X, y, lambda_reg=0, alpha=1e-2, epochs=5000, weights=None):\n",
    "    '''\n",
    "    Implementation of vectorized gradient descent with L2 regularization support\n",
    "    :param X: The input matrix N x M, where each row is an example\n",
    "    :param y: The output, N x 1\n",
    "    :param lambda_reg: Regularization hyperparamter (0 means no regularization)\n",
    "    :param alpha: Learning rate\n",
    "    :param epochs: Number of cycles over training data\n",
    "    :param weights: Initial weights can be supplied if desired\n",
    "    :return: The optimal weights after gradient descent\n",
    "    '''\n",
    "    weights = weights if weights is not None else np.random.uniform(high=10, size=[M_cols - 1])\n",
    "    N = len(X)\n",
    "    for epoch in range(epochs):\n",
    "        weights = weights - alpha / N * ( np.matmul(np.transpose(X), np.matmul(X, weights) - y) + lambda_reg * weights)\n",
    "\n",
    "    return weights\n",
    "\n",
    "def mean_square_error(X, y, W):\n",
    "    y_hat = np.matmul(X, W)\n",
    "    mean_square_err = np.sum(np.square(y - y_hat)) / len(y)\n",
    "\n",
    "    return mean_square_err\n",
    "\n",
    "\n",
    "def cross_validation_split(X, n_folds=5, filename=\"file\", write_to_csv=False):\n",
    "    '''\n",
    "    Splits the dataset intn n_folds\n",
    "    :param X: The input matrix, N x M\n",
    "    :param n_folds: The number of folds\n",
    "    :param filename: The output file prefix\n",
    "    :param write_to_csv: True if saving each fold required\n",
    "    :return: An array of dictionaries of length n_folds\n",
    "    '''\n",
    "    N = len(X) // n_folds\n",
    "    pairs = []\n",
    "    for i in range(n_folds):\n",
    "        fold_train1 = X[0:i * N]\n",
    "        if i < n_folds - 1:\n",
    "            fold_test = X[i*N:(i+1)*N]\n",
    "            fold_train2 = X[(i+1)*N:]\n",
    "        else:\n",
    "            fold_test = X[i*N:]\n",
    "            fold_train2 = X[N:N]\n",
    "\n",
    "        df_train = pd.DataFrame(np.concatenate((fold_train1, fold_train2)))\n",
    "        df_test = pd.DataFrame(fold_test)\n",
    "\n",
    "        if write_to_csv:\n",
    "            df_train.to_csv('./Datasets/' + filename + '-train' + str(i + 1) + '.csv', header=False, index=False)\n",
    "            df_test.to_csv('./Datasets/' + filename + '-test' + str(i + 1) + '.csv', header=False, index=False)\n",
    "\n",
    "        pairs.append({'train': df_train, 'test': df_test})\n",
    "\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def write_to_csv(to_df_object, filename, indexname=None):\n",
    "    df_to_csv = pd.DataFrame(to_df_object)\n",
    "    if indexname:\n",
    "        df_to_csv.index.name = indexname\n",
    "    df_to_csv.to_csv(filename + '.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the files for 5-fold cross-validation\n",
    "# splits = [{'train':<train_data>, 'test': <test_data>}, {'train':<train_data>, 'test': <test_data>}, {'train':<train_data>, 'test': <test_data>}\\\n",
    "# {'train':<train_data>, 'test': <test_data>}, {'train':<train_data>, 'test': <test_data>}]\n",
    "splits = cross_validation_split(data, 5, 'CandC', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSEs_closed_form = []\n",
    "all_weights = {}\n",
    "fold = 1\n",
    "for pair in splits:\n",
    "    X_train = pair['train'].drop([M_cols - 1], axis=1)\n",
    "    y_train = pair['train'][M_cols-1]\n",
    "    X_test = pair['test'].drop([M_cols - 1], axis=1)\n",
    "    y_test = pair['test'][M_cols - 1]\n",
    "\n",
    "    weights = fit(X_train, y_train)\n",
    "    MSEs_closed_form.append(mean_square_error(X_test, y_test, weights))\n",
    "    all_weights['fold' + str(fold)] = weights\n",
    "    fold += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSEs_gd = []\n",
    "all_weights_gd = {}\n",
    "weights = np.random.uniform(high=10., size=[M_cols - 1])\n",
    "fold = 1\n",
    "for pair in splits:\n",
    "    X_train = pair['train'].drop([M_cols - 1], axis=1)\n",
    "    y_train = pair['train'][M_cols - 1]\n",
    "    X_test = pair['test'].drop([M_cols - 1], axis=1)\n",
    "    y_test = pair['test'][M_cols - 1]\n",
    "\n",
    "    weights_gd = gradient_descent(np.array(X_train), np.array(y_train), epochs=20000, weights=np.copy(weights))\n",
    "    MSEs_gd.append(mean_square_error(X_test, y_test, weights_gd))\n",
    "    all_weights_gd['fold' + str(fold)] = weights_gd\n",
    "    fold += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "least squares: 0.27297450166938364\ngradient descent: 0.5878188923244896\n"
     ]
    }
   ],
   "source": [
    "print('least squares:', np.average(MSEs_closed_form))\n",
    "print('gradient descent:', np.average(MSEs_gd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The assignment wants us to store all these weights so the TAs can look over them.\n",
    "# Write weights to file (weight_index refers to w0, w1, etc. The weights for each fold are represented as columns in the csv. There are 5 columns and 123 rows)\n",
    "write_to_csv(all_weights, 'q3_part2_weights_for_closed_form', 'weight_index')\n",
    "write_to_csv(all_weights_gd, 'q3_part2_weights_for_gradient_descent', 'weight_index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best lambda: 1.4000000000000001\nMSE for best lambda: 0.018568358777022342\nMSE for lambda = 0.0 is 0.27297450166938364\nMSE for lambda = 0.05 is 0.018999415091165435\nMSE for lambda = 0.1 is 0.01887441111271968\nMSE for lambda = 0.15000000000000002 is 0.01880203759202171\nMSE for lambda = 0.2 is 0.01875359100671354\nMSE for lambda = 0.25 is 0.018718458808259363\nMSE for lambda = 0.30000000000000004 is 0.018691667370246367\nMSE for lambda = 0.35000000000000003 is 0.01867053166312279\nMSE for lambda = 0.4 is 0.018653455409163147\nMSE for lambda = 0.45 is 0.018639419677155204\nMSE for lambda = 0.5 is 0.018627737331107196\nMSE for lambda = 0.55 is 0.018617924263636742\nMSE for lambda = 0.6000000000000001 is 0.018609627062205678\nMSE for lambda = 0.65 is 0.018602580047813837\nMSE for lambda = 0.7000000000000001 is 0.018596578548643213\nMSE for lambda = 0.75 is 0.01859146160724642\nMSE for lambda = 0.8 is 0.018587100407000585\nMSE for lambda = 0.8500000000000001 is 0.018583390295462805\nMSE for lambda = 0.9 is 0.018580245143427057\nMSE for lambda = 0.9500000000000001 is 0.018577593264173863\nMSE for lambda = 1.0 is 0.018575374401377225\nMSE for lambda = 1.05 is 0.018573537465606742\nMSE for lambda = 1.1 is 0.018572038805897664\nMSE for lambda = 1.1500000000000001 is 0.01857084087079198\nMSE for lambda = 1.2000000000000002 is 0.018569911157572356\nMSE for lambda = 1.25 is 0.018569221377962554\nMSE for lambda = 1.3 is 0.01856874678864822\nMSE for lambda = 1.35 is 0.018568465648858834\nMSE for lambda = 1.4000000000000001 is 0.018568358777022342\nMSE for lambda = 1.4500000000000002 is 0.01856840918548216\nMSE for lambda = 1.5 is 0.018568601777306563\nMSE for lambda = 1.55 is 0.01856892309293693\nMSE for lambda = 1.6 is 0.01856936109714864\nMSE for lambda = 1.6500000000000001 is 0.018569904998885854\nMSE for lambda = 1.7000000000000002 is 0.018570545098077415\nMSE for lambda = 1.75 is 0.018571272654749116\nMSE for lambda = 1.8 is 0.018572079776662705\nMSE for lambda = 1.85 is 0.018572959322453914\nMSE for lambda = 1.9000000000000001 is 0.018573904817787687\nMSE for lambda = 1.9500000000000002 is 0.018574910382504396\nMSE for lambda = 2.0 is 0.018575970667100337\nMSE for lambda = 2.0500000000000003 is 0.018577080797151178\nMSE for lambda = 2.1 is 0.01857823632454106\nMSE for lambda = 2.15 is 0.018579433184527726\nMSE for lambda = 2.2 is 0.018580667657847327\nMSE for lambda = 2.25 is 0.018581936337172474\nMSE for lambda = 2.3000000000000003 is 0.018583236097350405\nMSE for lambda = 2.35 is 0.01858456406893257\nMSE for lambda = 2.4000000000000004 is 0.018585917614572314\nMSE for lambda = 2.45 is 0.01858729430794582\nMSE for lambda = 2.5 is 0.018588691914874014\nMSE for lambda = 2.5500000000000003 is 0.018590108376389518\nMSE for lambda = 2.6 is 0.018591541793519135\nMSE for lambda = 2.6500000000000004 is 0.018592990413580632\nMSE for lambda = 2.7 is 0.01859445261782456\nMSE for lambda = 2.75 is 0.018595926910267544\nMSE for lambda = 2.8000000000000003 is 0.018597411907588034\nMSE for lambda = 2.85 is 0.018598906329969345\nMSE for lambda = 2.9000000000000004 is 0.018600408992786498\nMSE for lambda = 2.95 is 0.018601918799054273\nMSE for lambda = 3.0 is 0.018603434732548944\nMSE for lambda = 3.0500000000000003 is 0.018604955851541934\nMSE for lambda = 3.1 is 0.01860648128308253\nMSE for lambda = 3.1500000000000004 is 0.018608010217771642\nMSE for lambda = 3.2 is 0.01860954190498418\nMSE for lambda = 3.25 is 0.018611075648492043\nMSE for lambda = 3.3000000000000003 is 0.018612610802452044\nMSE for lambda = 3.35 is 0.01861414676772357\nMSE for lambda = 3.4000000000000004 is 0.01861568298848641\nMSE for lambda = 3.45 is 0.018617218949128933\nMSE for lambda = 3.5 is 0.018618754171386656\nMSE for lambda = 3.5500000000000003 is 0.01862028821170146\nMSE for lambda = 3.6 is 0.018621820658791914\nMSE for lambda = 3.6500000000000004 is 0.018623351131405112\nMSE for lambda = 3.7 is 0.018624879276244177\nMSE for lambda = 3.75 is 0.018626404766050635\nMSE for lambda = 3.8000000000000003 is 0.01862792729783\nMSE for lambda = 3.85 is 0.01862944659120931\nMSE for lambda = 3.9000000000000004 is 0.018630962386914314\nMSE for lambda = 3.95 is 0.01863247444535853\nMSE for lambda = 4.0 is 0.01863398254533422\nMSE for lambda = 4.05 is 0.018635486482793905\nMSE for lambda = 4.1000000000000005 is 0.018636986069723353\nMSE for lambda = 4.15 is 0.018638481133088313\nMSE for lambda = 4.2 is 0.018639971513858274\nMSE for lambda = 4.25 is 0.018641457066096268\nMSE for lambda = 4.3 is 0.018642937656111698\nMSE for lambda = 4.3500000000000005 is 0.018644413161670315\nMSE for lambda = 4.4 is 0.018645883471256418\nMSE for lambda = 4.45 is 0.018647348483388348\nMSE for lambda = 4.5 is 0.01864880810597524\nMSE for lambda = 4.55 is 0.018650262255717885\nMSE for lambda = 4.6000000000000005 is 0.018651710857550602\nMSE for lambda = 4.65 is 0.01865315384411702\nMSE for lambda = 4.7 is 0.018654591155280016\nMSE for lambda = 4.75 is 0.018656022737666506\nMSE for lambda = 4.800000000000001 is 0.01865744854423699\nMSE for lambda = 4.8500000000000005 is 0.018658868533885015\nMSE for lambda = 4.9 is 0.018660282671060843\nMSE for lambda = 4.95 is 0.018661690925421102\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Part 3) Ridge-regression: Using only least-squares closed form solution for this part.\n",
    "'''\n",
    "INCREMENTS = 100\n",
    "MSE_vs_lambda = []\n",
    "lambdas = np.arange(0, 5, 5 / INCREMENTS)\n",
    "lowest_mse = 999999\n",
    "best_lambda = -1\n",
    "\n",
    "for lambda_value in lambdas:\n",
    "    cur_mse = 0\n",
    "    weights_for_folds = {}\n",
    "    fold = 0\n",
    "    for pair in splits:\n",
    "        X_train = pair['train'].drop([M_cols - 1], axis=1)\n",
    "        y_train = pair['train'][M_cols - 1]\n",
    "        X_test = pair['test'].drop([M_cols - 1], axis=1)\n",
    "        y_test = pair['test'][M_cols - 1]\n",
    "\n",
    "        weights = fit(X_train, y_train, lambda_value)\n",
    "        cur_mse += mean_square_error(X_test, y_test, weights)\n",
    "        weights_for_folds['fold' + str(fold)] = weights\n",
    "\n",
    "    cur_mse /= len(splits)\n",
    "    if cur_mse < lowest_mse:\n",
    "        lowest_mse = cur_mse\n",
    "        best_lambda = lambda_value\n",
    "\n",
    "    MSE_vs_lambda.append(cur_mse)\n",
    "    write_to_csv(weights_for_folds, './question3part3/weights_lambda_' + str(lambda_value), 'weight_index')\n",
    "\n",
    "print('Best lambda:', best_lambda)\n",
    "print('MSE for best lambda:', lowest_mse)\n",
    "\n",
    "for i in range(len(MSE_vs_lambda)):\n",
    "    print('MSE for lambda =', lambdas[i], 'is', MSE_vs_lambda[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x135f6050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "plt.plot(lambdas[1:], MSE_vs_lambda[1:])\n",
    "plt.title('Lambda vs MSE')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('MSE')\n",
    "plt.savefig('./plots/q3p31',  bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "# If a feature's associated weight is close to zero, it means that it contributes less to the prediction.\n",
    "# These features are good candidates for being excluded from the model.\n",
    "# We can first sort the weights and see which ones contribute the least to prediction\n",
    "all_weights_feature_selection = {}\n",
    "fold = 1\n",
    "for pair in splits:\n",
    "    X_train = pair['train'].drop([M_cols - 1], axis=1)\n",
    "    y_train = pair['train'][M_cols - 1]\n",
    "    X_test = pair['test'].drop([M_cols - 1], axis=1)\n",
    "    y_test = pair['test'][M_cols - 1]\n",
    "\n",
    "    weights = fit(X_train, y_train, lambda_reg=best_lambda) # Regularization with best_lambda from above\n",
    "    all_weights_feature_selection['fold' + str(fold)] = weights\n",
    "    fold += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(33, 0.0007658155985651137), (42, 0.0016334976077323961), (85, 0.002505190700459072), (119, 0.0025253638833119887), (36, 0.0027566131826181174), (67, 0.0038340661876292595), (21, 0.00538546464701382), (106, 0.005793980744495397), (97, 0.006059135062247073), (1, 0.006154077358450845), (118, 0.0065423607511759995), (53, 0.0070431642216821164), (56, 0.00791573095796599), (96, 0.009239222348098947), (20, 0.009464008902608479), (54, 0.010062298047947388), (28, 0.010144100139110507), (77, 0.010189363784573637), (82, 0.010244727184217272), (122, 0.01120187889305321), (58, 0.011225968724213118), (13, 0.011230909571080479), (108, 0.01211335987778007), (23, 0.013158690819599727), (98, 0.014273672865324764), (107, 0.01436820350509616), (61, 0.01499613869317994), (94, 0.015075070819671507), (104, 0.015363790356780262), (79, 0.015502461981422248), (27, 0.015621571418281013), (57, 0.016404121753609485), (115, 0.016469430394339804), (32, 0.0166616001349634), (121, 0.017398879874157815), (71, 0.018229975350721513), (47, 0.018517094688374175), (110, 0.02019379046052153), (93, 0.020421353746709486), (15, 0.023736293159884885), (112, 0.02547832456048617), (25, 0.027000066514438814), (84, 0.027328910868666404), (4, 0.02941926653089639), (44, 0.029819133796582138), (11, 0.029966167020273885), (117, 0.03186640509177722), (2, 0.03194290627420866), (24, 0.03202423696459303), (18, 0.032101641649181514), (114, 0.03254521535628735), (109, 0.03255920339019191), (100, 0.032605244083467426), (6, 0.03322660979881794), (48, 0.033268881036466996), (80, 0.03407768218331159), (116, 0.03428351340051471), (81, 0.03641944489419654), (101, 0.03654571253851449), (43, 0.037378895019438946), (102, 0.03875635643847463), (92, 0.03994351220787293), (74, 0.039975601747616385), (50, 0.039986597515467036), (12, 0.04269747561908552), (76, 0.04321478466078705), (105, 0.04365625890128068), (5, 0.04544883143296232), (14, 0.0458380548385219), (35, 0.047796169876625536), (55, 0.04839032811216587), (73, 0.04863134591852475), (9, 0.04948944945267633), (78, 0.05119492056538885), (120, 0.0513464413049394), (75, 0.05180592147472603), (95, 0.052555020726686705), (64, 0.0559030130545141), (66, 0.0559589644117726), (7, 0.056476598789494646), (41, 0.05707389904575982), (17, 0.05763134647683236), (26, 0.05783033348485977), (63, 0.06081844221005128), (59, 0.0617994765943733), (31, 0.06197678004426209), (38, 0.06239704158693065), (113, 0.06334511347745664), (89, 0.06542691690790196), (46, 0.06557541236981723), (68, 0.06607482142853355), (62, 0.06665520997055185), (87, 0.0679523098193506), (88, 0.0686970616736197), (37, 0.06953564678121404), (10, 0.0735460137932995), (60, 0.07363183913154155), (99, 0.07419311159422107), (111, 0.07419412642593778), (19, 0.07892315637705724), (0, 0.08105956602827247), (70, 0.08167524281071954), (22, 0.08367171178880166), (30, 0.09348973634317367), (52, 0.11141544657025726), (8, 0.11206084804413456), (16, 0.11291943487550304), (34, 0.11471754102852783), (49, 0.11529431166099365), (45, 0.11777499629806312), (103, 0.12075584232631024), (90, 0.1278943657935887), (65, 0.1301070708614127), (40, 0.1328306766317169), (29, 0.13332139710479984), (86, 0.13343373249625928), (39, 0.13861046732564525), (72, 0.14224164818898424), (91, 0.14492519236653226), (69, 0.1461153717801286), (83, 0.1589954337661779), (51, 0.16109442092806864), (3, 0.18689497308857925)]\n[(53, 0.00024511079082882624), (115, 0.0007558414177629432), (55, 0.0009530518684657238), (110, 0.0010782289064476588), (28, 0.0014143161288553369), (21, 0.0020800270620436622), (96, 0.002657360512323359), (67, 0.004447481263321526), (94, 0.005217460721242511), (93, 0.005256496005204229), (120, 0.006101483342743437), (78, 0.006149599023023013), (112, 0.006165203174467691), (97, 0.006587538377937042), (79, 0.006760068791709558), (98, 0.006791685112801554), (58, 0.0069444463832651175), (56, 0.007484793633180923), (81, 0.007678791261252772), (104, 0.008052441738328508), (111, 0.008219941211363154), (54, 0.009172091845173922), (61, 0.009699998314402231), (74, 0.011269595054951795), (47, 0.01141742055497122), (7, 0.011851835118481618), (108, 0.014719259304104528), (33, 0.014728827668512984), (36, 0.01583776101087505), (119, 0.016730825247275895), (113, 0.017752544401095888), (27, 0.01808552305728883), (106, 0.018765433076359903), (57, 0.019423617116723173), (23, 0.020159885877744362), (42, 0.021009387297470337), (43, 0.021075141507678963), (107, 0.02116759277612194), (84, 0.022854550044318665), (71, 0.02378763274151534), (85, 0.024152657401235032), (48, 0.02471982105700601), (5, 0.025952336054964963), (82, 0.026596995583569455), (2, 0.026649217489696092), (116, 0.02683384212659855), (60, 0.026890304200492984), (52, 0.029113597316509796), (64, 0.029721686556977173), (18, 0.029828947799300305), (25, 0.03009568151829302), (114, 0.0313719330747436), (24, 0.034611679889737026), (41, 0.03483030892813191), (38, 0.03547241847605021), (15, 0.03588316502353519), (6, 0.03686189745189787), (59, 0.03714541423435442), (46, 0.037629435768836865), (26, 0.03893189078110328), (66, 0.04031951627767486), (121, 0.040327536207621986), (77, 0.04233942853582992), (12, 0.04308050426671671), (88, 0.04506664052594506), (1, 0.045303797297455276), (87, 0.045648661020105445), (4, 0.04578251214310058), (13, 0.04620356611683992), (95, 0.04894582277473348), (76, 0.04913653512524151), (10, 0.04929011380139647), (118, 0.05039519144502202), (73, 0.0515166189666269), (20, 0.051914002148253244), (9, 0.05252611122299313), (101, 0.053050232264682334), (100, 0.05390139919288143), (80, 0.05400394649703637), (37, 0.05459773428994879), (35, 0.05852813798811542), (50, 0.05901921403474208), (103, 0.059891001803363327), (32, 0.06120473093456943), (102, 0.06411119605385114), (11, 0.06855526763105146), (44, 0.06860872837049552), (105, 0.06967879941980978), (19, 0.06982186508780704), (75, 0.07019685767100257), (30, 0.07088687428918855), (70, 0.07422598033423415), (31, 0.07591801309491593), (63, 0.07962235697057861), (8, 0.08039236346370406), (109, 0.08075857076892788), (122, 0.08261015639038152), (45, 0.08465545167867017), (89, 0.08559846921528924), (68, 0.08904957467550022), (92, 0.08935148133744894), (90, 0.09299684702909056), (99, 0.0982974231400202), (69, 0.1059637192820851), (0, 0.10970836276966837), (29, 0.11077252040329827), (39, 0.11497260713077591), (14, 0.11618129930827893), (117, 0.11812245694858067), (62, 0.12061957587950205), (17, 0.12337493018661695), (49, 0.12842234633286354), (22, 0.1290489173069225), (16, 0.1317401882112779), (91, 0.13611738151676483), (34, 0.13669464796703537), (72, 0.13860038094914923), (40, 0.14170202501002976), (65, 0.14459085209355), (86, 0.164407383698536), (3, 0.1702672917279699), (51, 0.1742808967682731), (83, 0.18141786793889444)]\n[(98, 0.00042183408853387614), (33, 0.00042808147568147085), (110, 0.00044243110048017466), (104, 0.0004570856910852658), (55, 0.001338693398863387), (94, 0.0018027586955106547), (50, 0.0032046282545695063), (5, 0.003998694133155749), (61, 0.004658552783679179), (113, 0.005662483048156112), (84, 0.0059395854350330944), (60, 0.006481242756045697), (108, 0.0066297207065999075), (56, 0.007063214840813898), (122, 0.007663662368668936), (47, 0.007751480143287348), (93, 0.008061095832640178), (42, 0.010045583393296796), (79, 0.01064862387643109), (11, 0.011506828474800734), (97, 0.011787265718046737), (48, 0.012275118960946824), (77, 0.01267737409998981), (32, 0.012739708889624744), (115, 0.012914564604325223), (36, 0.01436626236888749), (81, 0.014904062404374113), (96, 0.014926343888953008), (21, 0.015466032201533818), (25, 0.01596766784993765), (112, 0.016073792991738202), (18, 0.01703869145070934), (27, 0.017063836828519516), (82, 0.01856840764493868), (85, 0.018603170522179815), (88, 0.018916422898222995), (107, 0.020066279229184112), (23, 0.022170670675564004), (120, 0.022398760748389306), (74, 0.023022566287738723), (54, 0.023057117667171168), (71, 0.023112497971254836), (114, 0.023584831696299234), (28, 0.024965615790109807), (7, 0.029163842939731482), (58, 0.02956959440359872), (106, 0.030715399536625346), (102, 0.03139444813713217), (121, 0.03213941163930411), (24, 0.033159144972710754), (78, 0.03362542773984791), (117, 0.03493190029056689), (43, 0.03560399752133879), (4, 0.035723766661571114), (26, 0.036743233434577194), (12, 0.037001287488378214), (67, 0.03738542268843781), (46, 0.03843984744842493), (1, 0.03925478199016977), (15, 0.04038994736344849), (95, 0.04063402233548667), (105, 0.04128617820871233), (63, 0.04252039197861637), (37, 0.04454284201163423), (116, 0.04463344932392455), (2, 0.04530847419183503), (111, 0.04700130663862763), (87, 0.04725971342331568), (9, 0.047748593942595514), (38, 0.047966387687042145), (31, 0.048352398234616546), (53, 0.0487628510226986), (17, 0.049610340009954214), (118, 0.050100475484229835), (109, 0.0501428727878675), (6, 0.05227410357640276), (13, 0.05502096401853192), (100, 0.05852795605021937), (76, 0.05873447543280064), (35, 0.05937827639209365), (101, 0.0603220821863923), (19, 0.0618230052314062), (73, 0.0621722080587273), (119, 0.06307805814809245), (66, 0.06363636566309579), (30, 0.06597340739549981), (57, 0.0681551535648081), (75, 0.06832184623780105), (80, 0.06845897987912546), (64, 0.07231954663837599), (68, 0.07554594727493662), (20, 0.07749575627352487), (14, 0.08059772145738592), (70, 0.08321485176625926), (44, 0.08366065735081808), (41, 0.08402341582225265), (59, 0.08688570715300942), (29, 0.08807170253334401), (103, 0.08834831247593236), (89, 0.08987891247249057), (52, 0.09350999380080038), (92, 0.09676505311053288), (10, 0.099004534044591), (90, 0.10085519190982233), (22, 0.1035956968917772), (99, 0.11108753803467245), (40, 0.11233953314272804), (62, 0.1275943428585301), (72, 0.1310253029239249), (0, 0.13226163289604917), (49, 0.13440194047572337), (8, 0.134868140608659), (65, 0.1394972273630369), (45, 0.14145599201739972), (69, 0.14777339398604397), (3, 0.15543450587130342), (86, 0.15547500884532722), (39, 0.156167091701493), (16, 0.15671194399979682), (34, 0.16726713563410572), (51, 0.16919947988611828), (83, 0.17183013359231616), (91, 0.18070209420664335)]\n[(96, 9.290876738517933e-05), (57, 0.0001283702656979703), (107, 0.0006528618766182064), (58, 0.0021843107190118475), (5, 0.002192706412451704), (84, 0.004576906500141405), (113, 0.005004406252841256), (93, 0.005110967736213135), (1, 0.005654351159181001), (119, 0.006759580985286832), (115, 0.008716707650196678), (102, 0.009593488535340332), (67, 0.010610132633370564), (55, 0.012698240628247918), (82, 0.013382235805066176), (71, 0.013448098409357886), (32, 0.01367857052322442), (50, 0.013753377087309239), (42, 0.013993011742836512), (60, 0.01454238877572768), (112, 0.01581984044005199), (36, 0.01585095776138889), (79, 0.016096063976774398), (53, 0.016421014949547338), (25, 0.01646382287601878), (47, 0.01693788395067972), (56, 0.0190179679259392), (94, 0.019242112293805826), (88, 0.019788820990136936), (54, 0.020838074318149883), (74, 0.021377128752284695), (95, 0.022445680528958126), (118, 0.022714388177095583), (33, 0.02406934008748033), (97, 0.024247094600266583), (23, 0.026033462705833354), (28, 0.026088491963391414), (31, 0.02678364804719349), (24, 0.027005203452345902), (101, 0.027124791600414134), (100, 0.027292885707734634), (4, 0.02746870938648599), (81, 0.027829369326430445), (2, 0.027939559359591585), (75, 0.02871520566471789), (13, 0.029085131504148192), (44, 0.029457610136461546), (120, 0.03022892597858346), (77, 0.03229580639233569), (78, 0.03304538669987043), (21, 0.03325537375232501), (66, 0.03423457474336827), (15, 0.03524798821472424), (48, 0.03538977467623939), (59, 0.0376537297977805), (85, 0.03817067989664404), (106, 0.03933343904817686), (121, 0.03966915500442807), (114, 0.03978171501603453), (35, 0.042723039342645215), (61, 0.04364215071587534), (27, 0.04387371901591942), (43, 0.04420364081725973), (26, 0.044438956695401), (116, 0.04504418055840633), (11, 0.04554366890618645), (12, 0.04579765521052408), (109, 0.04588064234258946), (110, 0.05127808665903301), (76, 0.05175931479769519), (103, 0.052265496279516786), (7, 0.05394114327886065), (63, 0.05534846481123428), (20, 0.0558931953023352), (37, 0.05685502114035386), (18, 0.0577321355786723), (46, 0.05852503167145855), (9, 0.05856928640657998), (111, 0.06118897829439423), (117, 0.06252363448802205), (14, 0.06437236965354938), (98, 0.0648459342327728), (38, 0.06501403991687704), (64, 0.06562219525325337), (104, 0.0660847004609155), (87, 0.07118884976929876), (73, 0.07125678005912486), (69, 0.07915336726195125), (122, 0.08004969491924795), (10, 0.08009876771442305), (99, 0.08120052111730662), (108, 0.08265971141311539), (68, 0.08298831742945018), (19, 0.08321876738569522), (105, 0.08478000288904128), (41, 0.08731694312170009), (80, 0.08794754326063892), (92, 0.0896009524097578), (29, 0.09133085437317803), (52, 0.09299285700472104), (17, 0.09583340503617312), (22, 0.0971281259560324), (89, 0.09810299231277521), (30, 0.09849789094460429), (70, 0.10086045716158433), (62, 0.10136241692858704), (90, 0.10822707726855492), (6, 0.11412461114797665), (40, 0.11651883918045002), (0, 0.11760220843607826), (16, 0.11785277190671879), (72, 0.12283021964044026), (8, 0.1260566272541615), (86, 0.1287207323586297), (34, 0.13949144086862625), (49, 0.14106093858746965), (91, 0.15270883319460052), (83, 0.15443956461099106), (51, 0.15829022822844974), (65, 0.1622785206139581), (39, 0.16943774810719772), (45, 0.17056319167342088), (3, 0.1755521193736908)]\n[(9, 0.0001051542966236436), (117, 0.0012018489092911146), (78, 0.0013444323438525935), (110, 0.0019639673375279657), (7, 0.002623458830453012), (98, 0.003080562676844112), (88, 0.0034617492950668907), (28, 0.0035248888100767326), (42, 0.004174850321252138), (1, 0.004222049171748551), (104, 0.004224347216037018), (113, 0.004338606170614527), (93, 0.006241467567891536), (119, 0.0064120265768639224), (71, 0.006525565511865298), (102, 0.008115402274130176), (60, 0.008317378234103907), (115, 0.009019331583070016), (107, 0.009795493371976018), (121, 0.010166044640001431), (67, 0.010349778177228649), (81, 0.011482995380235061), (111, 0.011577063839341687), (61, 0.012716137594777136), (57, 0.012896951135904035), (24, 0.013508766905702818), (96, 0.013810610642906063), (112, 0.01521158786566396), (11, 0.016892332523575075), (108, 0.018516795025718236), (50, 0.01864325904699955), (74, 0.01882828497590504), (105, 0.02137999689158959), (77, 0.022715952665965454), (48, 0.023278385971175576), (97, 0.023590812528230778), (54, 0.02371182059050243), (85, 0.02404931602559366), (114, 0.024552274428915614), (2, 0.02477479262622379), (95, 0.024946056796992296), (5, 0.025082804164404057), (79, 0.02519393916610556), (14, 0.02697217658077014), (33, 0.028734618448247443), (84, 0.028990202814253126), (47, 0.029383502002820656), (106, 0.030992687435405782), (116, 0.03180028306078689), (38, 0.03240484921436933), (26, 0.03261492529527147), (53, 0.03286440120250088), (41, 0.03303015501249183), (21, 0.033263750541737885), (46, 0.03379283205048941), (82, 0.03424765807731026), (118, 0.03497098088363643), (27, 0.03572570376208201), (94, 0.03572704010078833), (59, 0.03581362497690297), (36, 0.03586179166594469), (15, 0.03667167286397016), (12, 0.03984612921634942), (58, 0.04016986036552436), (23, 0.04033163355787854), (35, 0.0423960758032654), (20, 0.04275345175395056), (63, 0.042764137767827816), (103, 0.04322627331769895), (66, 0.04438581131958427), (13, 0.04526729284178869), (37, 0.04542179447484277), (56, 0.04871371717224766), (25, 0.049291762318947475), (120, 0.04937330868466904), (18, 0.05219240480430072), (19, 0.05395445651249477), (55, 0.05431326643117822), (75, 0.05859557513967715), (109, 0.05891406625386892), (64, 0.05903365501274008), (122, 0.06093829458892749), (17, 0.06179208948760198), (73, 0.06239032027212872), (4, 0.06290797694415494), (100, 0.0634611085804351), (89, 0.06352887889093528), (101, 0.06471801481623499), (44, 0.06489027009834476), (87, 0.06710561338667376), (76, 0.07007704803242064), (6, 0.07069078921062405), (43, 0.07101275697065743), (70, 0.07633181049359984), (52, 0.07665910046622969), (31, 0.07972181938250347), (92, 0.08155132569172938), (30, 0.0835919741763149), (16, 0.08399593254179764), (80, 0.08733359656715622), (62, 0.0874607868743744), (68, 0.08923733655049286), (32, 0.08954270374007402), (99, 0.09227149663347972), (40, 0.0941977399731978), (10, 0.1047508130321816), (8, 0.10660666663776834), (72, 0.10699262327120707), (90, 0.11146868772619427), (29, 0.11801589419239647), (49, 0.1200470137754073), (86, 0.12239352073133744), (34, 0.12368719045409947), (69, 0.1272123818215706), (51, 0.12940125421540977), (45, 0.12988655085768422), (0, 0.13057756515026228), (65, 0.13168448322935594), (39, 0.13199135548203822), (22, 0.13875270597047384), (91, 0.1406096625175841), (83, 0.175674156845533), (3, 0.17902346068514413)]\n"
     ]
    }
   ],
   "source": [
    "# Now we look for weights that are close to zero in the regularized fit\n",
    "features_to_drop = set()\n",
    "for key in all_weights_feature_selection:\n",
    "    w = all_weights_feature_selection[key]\n",
    "    # This basically converts the weight vector [w0, w1...] to [(0, w0), (1, w1),...] so after we sort the weights we know which features they correspond to\n",
    "    all_weights_feature_selection[key] = [(i, abs(w[i])) for i in range(len(w))]\n",
    "    all_weights_feature_selection[key].sort(key=lambda x: x[1])\n",
    "    [features_to_drop.add(x[0]) for x in all_weights_feature_selection[key][:20]]\n",
    "\n",
    "for val in all_weights_feature_selection.values():\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features to be dropped: 51\nBest lambda: 1.4500000000000002\nMSE for best lambda: 0.01840859569971876\nMSE for best lambda after dropping least contributing features: 0.01791864486507244\n"
     ]
    }
   ],
   "source": [
    "MSEs_final = []\n",
    "all_weights_final = {}\n",
    "fold = 1\n",
    "for pair in splits:\n",
    "    X_train = pair['train'].drop([M_cols - 1], axis=1)\n",
    "    y_train = pair['train'][M_cols-1]\n",
    "    X_test = pair['test'].drop([M_cols - 1], axis=1)\n",
    "    y_test = pair['test'][M_cols - 1]\n",
    "\n",
    "    # Drop the features we found to have near 0 coefficients\n",
    "    X_train.drop(list(features_to_drop), axis=1, inplace=True)\n",
    "    X_test.drop(list(features_to_drop), axis=1, inplace=True)\n",
    "\n",
    "    weights = fit(X_train, y_train, lambda_reg=best_lambda)\n",
    "    MSEs_final.append(mean_square_error(X_test, y_test, weights))\n",
    "    all_weights_final['fold' + str(fold)] = weights\n",
    "    fold += 1\n",
    "\n",
    "print('Number of features to be dropped:', len(features_to_drop))\n",
    "print('Best lambda:', best_lambda)\n",
    "print('MSE for best lambda:', lowest_mse)\n",
    "print('MSE for best lambda after dropping least contributing features:', np.average(MSEs_final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
